{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax exercise\n",
    "\n",
    "*Complete and hand in this completed worksheet (including its outputs and any supporting code outside of the worksheet) with your assignment submission. For more details see the [assignments page](http://vision.stanford.edu/teaching/cs231n/assignments.html) on the course website.*\n",
    "\n",
    "This exercise is analogous to the SVM exercise. You will:\n",
    "\n",
    "- implement a fully-vectorized **loss function** for the Softmax classifier\n",
    "- implement the fully-vectorized expression for its **analytic gradient**\n",
    "- **check your implementation** with numerical gradient\n",
    "- use a validation set to **tune the learning rate and regularization** strength\n",
    "- **optimize** the loss function with **SGD**\n",
    "- **visualize** the final learned weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from cs231n.data_utils import load_CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading extenrnal modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 3073)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 3073)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (1000, 3073)\n",
      "Test labels shape:  (1000,)\n",
      "dev data shape:  (500, 3073)\n",
      "dev labels shape:  (500,)\n"
     ]
    }
   ],
   "source": [
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000, num_dev=500):\n",
    "    \"\"\"\n",
    "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "    it for the linear classifier. These are the same steps as we used for the\n",
    "    SVM, but condensed to a single function.  \n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'\n",
    "    \n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "    \n",
    "    # subsample the data\n",
    "    mask = list(range(num_training, num_training + num_validation))\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = list(range(num_training))\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = list(range(num_test))\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "    mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "    X_dev = X_train[mask]\n",
    "    y_dev = y_train[mask]\n",
    "    \n",
    "    # Preprocessing: reshape the image data into rows\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "    X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "    X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n",
    "    \n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis = 0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "    X_dev -= mean_image\n",
    "    \n",
    "    # add bias dimension and transform into columns\n",
    "    X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "    X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "    X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "    X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev\n",
    "\n",
    "\n",
    "# Cleaning up variables to prevent loading data multiple times (which may cause memory issue)\n",
    "try:\n",
    "   del X_train, y_train\n",
    "   del X_test, y_test\n",
    "   print('Clear previously loaded data.')\n",
    "except:\n",
    "   pass\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev = get_CIFAR10_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)\n",
    "print('dev data shape: ', X_dev.shape)\n",
    "print('dev labels shape: ', y_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Classifier\n",
    "\n",
    "Your code for this section will all be written inside **cs231n/classifiers/softmax.py**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.287783\n",
      "sanity check: 2.302585\n"
     ]
    }
   ],
   "source": [
    "# First implement the naive softmax loss function with nested loops.\n",
    "# Open the file cs231n/classifiers/softmax.py and implement the\n",
    "# softmax_loss_naive function.\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_naive\n",
    "import time\n",
    "\n",
    "# Generate a random softmax weight matrix and use it to compute the loss.\n",
    "W = np.random.randn(3073, 10) * 0.0001\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As a rough sanity check, our loss should be something close to -log(0.1).\n",
    "print('loss: %f' % loss)\n",
    "print('sanity check: %f' % (-np.log(0.1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inline Question 1:\n",
    "Why do we expect our loss to be close to -log(0.1)? Explain briefly.**\n",
    "\n",
    "**Your answer:** *Fill this in*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: 1.881633 analytic: 1.881633, relative error: 3.993444e-09\n",
      "numerical: -3.444678 analytic: -3.444678, relative error: 2.490449e-08\n",
      "numerical: 0.466383 analytic: 0.466383, relative error: 7.026472e-08\n",
      "numerical: -3.986919 analytic: -3.986919, relative error: 6.392515e-09\n",
      "numerical: 0.694917 analytic: 0.694917, relative error: 2.690912e-08\n",
      "numerical: -0.653553 analytic: -0.653553, relative error: 2.304506e-08\n",
      "numerical: -3.034804 analytic: -3.034804, relative error: 1.808639e-08\n",
      "numerical: 2.038139 analytic: 2.038139, relative error: 3.533002e-08\n",
      "numerical: -0.163462 analytic: -0.163462, relative error: 2.333861e-07\n",
      "numerical: -0.897415 analytic: -0.897416, relative error: 4.261251e-08\n",
      "numerical: 0.617762 analytic: 0.617762, relative error: 3.038684e-08\n",
      "numerical: -0.365761 analytic: -0.365761, relative error: 1.563491e-07\n",
      "numerical: 1.722502 analytic: 1.722502, relative error: 1.123827e-08\n",
      "numerical: 0.397004 analytic: 0.397004, relative error: 3.599397e-08\n",
      "numerical: -0.641094 analytic: -0.641094, relative error: 3.646338e-08\n",
      "numerical: -0.601486 analytic: -0.601486, relative error: 1.554654e-07\n",
      "numerical: -0.867204 analytic: -0.867205, relative error: 3.444139e-08\n",
      "numerical: -1.581229 analytic: -1.581229, relative error: 2.286766e-08\n",
      "numerical: 0.177508 analytic: 0.177508, relative error: 4.125244e-08\n",
      "numerical: -0.835552 analytic: -0.835552, relative error: 4.932958e-08\n"
     ]
    }
   ],
   "source": [
    "# Complete the implementation of softmax_loss_naive and implement a (naive)\n",
    "# version of the gradient that uses nested loops.\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As we did for the SVM, use numeric gradient checking as a debugging tool.\n",
    "# The numeric gradient should be close to the analytic gradient.\n",
    "from cs231n.gradient_check import grad_check_sparse\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 0.0)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)\n",
    "\n",
    "# similar to SVM case, do another gradient check with regularization\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 5e1)\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 5e1)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive loss: 2.287783e+00 computed in 0.173581s\n",
      "vectorized loss: 2.287783e+00 computed in 0.006943s\n",
      "Loss difference: 0.000000\n",
      "Gradient difference: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Now that we have a naive implementation of the softmax loss function and its gradient,\n",
    "# implement a vectorized version in softmax_loss_vectorized.\n",
    "# The two versions should compute the same results, but the vectorized version should be\n",
    "# much faster.\n",
    "tic = time.time()\n",
    "loss_naive, grad_naive = softmax_loss_naive(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('naive loss: %e computed in %fs' % (loss_naive, toc - tic))\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_vectorized\n",
    "tic = time.time()\n",
    "loss_vectorized, grad_vectorized = softmax_loss_vectorized(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('vectorized loss: %e computed in %fs' % (loss_vectorized, toc - tic))\n",
    "\n",
    "# As we did for the SVM, we use the Frobenius norm to compare the two versions\n",
    "# of the gradient.\n",
    "grad_difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')\n",
    "print('Loss difference: %f' % np.abs(loss_naive - loss_vectorized))\n",
    "print('Gradient difference: %f' % grad_difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 1000: loss 158.834264\n",
      "iteration 100 / 1000: loss 142.923278\n",
      "iteration 200 / 1000: loss 129.364645\n",
      "iteration 300 / 1000: loss 116.755987\n",
      "iteration 400 / 1000: loss 105.664982\n",
      "iteration 500 / 1000: loss 95.534248\n",
      "iteration 600 / 1000: loss 86.326744\n",
      "iteration 700 / 1000: loss 78.457382\n",
      "iteration 800 / 1000: loss 71.048170\n",
      "iteration 900 / 1000: loss 64.438601\n",
      "(5e-08, 5000.0)\n",
      "(0.21806122448979592, 0.253)\n",
      "\n",
      "iteration 0 / 1000: loss 250.797694\n",
      "iteration 100 / 1000: loss 211.803183\n",
      "iteration 200 / 1000: loss 180.221369\n",
      "iteration 300 / 1000: loss 153.587857\n",
      "iteration 400 / 1000: loss 130.883330\n",
      "iteration 500 / 1000: loss 111.691908\n",
      "iteration 600 / 1000: loss 95.442961\n",
      "iteration 700 / 1000: loss 81.368330\n",
      "iteration 800 / 1000: loss 69.769615\n",
      "iteration 900 / 1000: loss 59.450425\n",
      "(5e-08, 8000.0)\n",
      "(0.25018367346938775, 0.256)\n",
      "\n",
      "iteration 0 / 1000: loss 345.483189\n",
      "iteration 100 / 1000: loss 276.491414\n",
      "iteration 200 / 1000: loss 221.928127\n",
      "iteration 300 / 1000: loss 178.321929\n",
      "iteration 400 / 1000: loss 143.262637\n",
      "iteration 500 / 1000: loss 115.182369\n",
      "iteration 600 / 1000: loss 92.826262\n",
      "iteration 700 / 1000: loss 74.641477\n",
      "iteration 800 / 1000: loss 60.329937\n",
      "iteration 900 / 1000: loss 48.792970\n",
      "(5e-08, 11000.0)\n",
      "(0.2593265306122449, 0.278)\n",
      "\n",
      "iteration 0 / 1000: loss 435.208902\n",
      "iteration 100 / 1000: loss 328.802406\n",
      "iteration 200 / 1000: loss 248.424253\n",
      "iteration 300 / 1000: loss 187.806909\n",
      "iteration 400 / 1000: loss 142.328395\n",
      "iteration 500 / 1000: loss 107.742834\n",
      "iteration 600 / 1000: loss 81.822610\n",
      "iteration 700 / 1000: loss 62.283827\n",
      "iteration 800 / 1000: loss 47.417431\n",
      "iteration 900 / 1000: loss 36.464824\n",
      "(5e-08, 14000.0)\n",
      "(0.26985714285714285, 0.275)\n",
      "\n",
      "iteration 0 / 1000: loss 533.258171\n",
      "iteration 100 / 1000: loss 378.849822\n",
      "iteration 200 / 1000: loss 269.798548\n",
      "iteration 300 / 1000: loss 192.130159\n",
      "iteration 400 / 1000: loss 137.166507\n",
      "iteration 500 / 1000: loss 98.052867\n",
      "iteration 600 / 1000: loss 70.355736\n",
      "iteration 700 / 1000: loss 50.627627\n",
      "iteration 800 / 1000: loss 36.591226\n",
      "iteration 900 / 1000: loss 26.493287\n",
      "(5e-08, 17000.0)\n",
      "(0.29185714285714287, 0.298)\n",
      "\n",
      "iteration 0 / 1000: loss 618.961970\n",
      "iteration 100 / 1000: loss 414.665288\n",
      "iteration 200 / 1000: loss 277.849070\n",
      "iteration 300 / 1000: loss 186.594138\n",
      "iteration 400 / 1000: loss 125.724837\n",
      "iteration 500 / 1000: loss 84.677417\n",
      "iteration 600 / 1000: loss 57.431192\n",
      "iteration 700 / 1000: loss 39.065717\n",
      "iteration 800 / 1000: loss 26.827244\n",
      "iteration 900 / 1000: loss 18.700522\n",
      "(5e-08, 20000.0)\n",
      "(0.3017755102040816, 0.319)\n",
      "\n",
      "iteration 0 / 1000: loss 713.575172\n",
      "iteration 100 / 1000: loss 449.930844\n",
      "iteration 200 / 1000: loss 283.969094\n",
      "iteration 300 / 1000: loss 179.607315\n",
      "iteration 400 / 1000: loss 114.059155\n",
      "iteration 500 / 1000: loss 72.496308\n",
      "iteration 600 / 1000: loss 46.441026\n",
      "iteration 700 / 1000: loss 30.056206\n",
      "iteration 800 / 1000: loss 19.693795\n",
      "iteration 900 / 1000: loss 13.177049\n",
      "(5e-08, 23000.0)\n",
      "(0.297734693877551, 0.301)\n",
      "\n",
      "iteration 0 / 1000: loss 806.040548\n",
      "iteration 100 / 1000: loss 478.474796\n",
      "iteration 200 / 1000: loss 284.721698\n",
      "iteration 300 / 1000: loss 169.821938\n",
      "iteration 400 / 1000: loss 101.534682\n",
      "iteration 500 / 1000: loss 61.164087\n",
      "iteration 600 / 1000: loss 37.076912\n",
      "iteration 700 / 1000: loss 22.716828\n",
      "iteration 800 / 1000: loss 14.433776\n",
      "iteration 900 / 1000: loss 9.377217\n",
      "(5e-08, 26000.0)\n",
      "(0.30981632653061225, 0.32)\n",
      "\n",
      "iteration 0 / 1000: loss 906.144404\n",
      "iteration 100 / 1000: loss 506.435561\n",
      "iteration 200 / 1000: loss 283.700147\n",
      "iteration 300 / 1000: loss 159.420782\n",
      "iteration 400 / 1000: loss 89.964836\n",
      "iteration 500 / 1000: loss 51.146475\n",
      "iteration 600 / 1000: loss 29.492886\n",
      "iteration 700 / 1000: loss 17.501118\n",
      "iteration 800 / 1000: loss 10.668354\n",
      "iteration 900 / 1000: loss 6.858504\n",
      "(5e-08, 29000.0)\n",
      "(0.30951020408163266, 0.311)\n",
      "\n",
      "iteration 0 / 1000: loss 964.038746\n",
      "iteration 100 / 1000: loss 508.109988\n",
      "iteration 200 / 1000: loss 267.912685\n",
      "iteration 300 / 1000: loss 141.796073\n",
      "iteration 400 / 1000: loss 75.601610\n",
      "iteration 500 / 1000: loss 40.740504\n",
      "iteration 600 / 1000: loss 22.418439\n",
      "iteration 700 / 1000: loss 12.822709\n",
      "iteration 800 / 1000: loss 7.750615\n",
      "iteration 900 / 1000: loss 5.083106\n",
      "(5e-08, 32000.0)\n",
      "(0.31151020408163266, 0.313)\n",
      "\n",
      "iteration 0 / 1000: loss 1084.788254\n",
      "iteration 100 / 1000: loss 538.180424\n",
      "iteration 200 / 1000: loss 267.533990\n",
      "iteration 300 / 1000: loss 133.520778\n",
      "iteration 400 / 1000: loss 67.170116\n",
      "iteration 500 / 1000: loss 34.337567\n",
      "iteration 600 / 1000: loss 18.070339\n",
      "iteration 700 / 1000: loss 10.050325\n",
      "iteration 800 / 1000: loss 6.010718\n",
      "iteration 900 / 1000: loss 4.000623\n",
      "(5e-08, 35000.0)\n",
      "(0.31726530612244896, 0.333)\n",
      "\n",
      "iteration 0 / 1000: loss 1165.194646\n",
      "iteration 100 / 1000: loss 544.056082\n",
      "iteration 200 / 1000: loss 254.549948\n",
      "iteration 300 / 1000: loss 119.867410\n",
      "iteration 400 / 1000: loss 56.968588\n",
      "iteration 500 / 1000: loss 27.735978\n",
      "iteration 600 / 1000: loss 14.062009\n",
      "iteration 700 / 1000: loss 7.691442\n",
      "iteration 800 / 1000: loss 4.729029\n",
      "iteration 900 / 1000: loss 3.307397\n",
      "(5e-08, 38000.0)\n",
      "(0.3153469387755102, 0.328)\n",
      "\n",
      "iteration 0 / 1000: loss 1267.970579\n",
      "iteration 100 / 1000: loss 557.794596\n",
      "iteration 200 / 1000: loss 246.055469\n",
      "iteration 300 / 1000: loss 109.302981\n",
      "iteration 400 / 1000: loss 49.117323\n",
      "iteration 500 / 1000: loss 22.818600\n",
      "iteration 600 / 1000: loss 11.144238\n",
      "iteration 700 / 1000: loss 6.115067\n",
      "iteration 800 / 1000: loss 3.831716\n",
      "iteration 900 / 1000: loss 2.918400\n",
      "(5e-08, 41000.0)\n",
      "(0.31295918367346937, 0.325)\n",
      "\n",
      "iteration 0 / 1000: loss 1352.148979\n",
      "iteration 100 / 1000: loss 560.171857\n",
      "iteration 200 / 1000: loss 232.652750\n",
      "iteration 300 / 1000: loss 97.422531\n",
      "iteration 400 / 1000: loss 41.503047\n",
      "iteration 500 / 1000: loss 18.498682\n",
      "iteration 600 / 1000: loss 8.847393\n",
      "iteration 700 / 1000: loss 4.862932\n",
      "iteration 800 / 1000: loss 3.293562\n",
      "iteration 900 / 1000: loss 2.575391\n",
      "(5e-08, 44000.0)\n",
      "(0.31051020408163266, 0.323)\n",
      "\n",
      "iteration 0 / 1000: loss 1449.591444\n",
      "iteration 100 / 1000: loss 565.369771\n",
      "iteration 200 / 1000: loss 221.511026\n",
      "iteration 300 / 1000: loss 87.480175\n",
      "iteration 400 / 1000: loss 35.311820\n",
      "iteration 500 / 1000: loss 15.053224\n",
      "iteration 600 / 1000: loss 7.194960\n",
      "iteration 700 / 1000: loss 4.062474\n",
      "iteration 800 / 1000: loss 2.915772\n",
      "iteration 900 / 1000: loss 2.431295\n",
      "(5e-08, 47000.0)\n",
      "(0.31326530612244896, 0.325)\n",
      "\n",
      "iteration 0 / 1000: loss 1548.774005\n",
      "iteration 100 / 1000: loss 568.286009\n",
      "iteration 200 / 1000: loss 209.509069\n",
      "iteration 300 / 1000: loss 78.042020\n",
      "iteration 400 / 1000: loss 29.909256\n",
      "iteration 500 / 1000: loss 12.359619\n",
      "iteration 600 / 1000: loss 5.875329\n",
      "iteration 700 / 1000: loss 3.506126\n",
      "iteration 800 / 1000: loss 2.621144\n",
      "iteration 900 / 1000: loss 2.302906\n",
      "(5e-08, 50000.0)\n",
      "(0.30479591836734693, 0.327)\n",
      "\n",
      "iteration 0 / 1000: loss 161.179728\n",
      "iteration 100 / 1000: loss 130.895827\n",
      "iteration 200 / 1000: loss 106.921777\n",
      "iteration 300 / 1000: loss 87.576084\n",
      "iteration 400 / 1000: loss 71.810323\n",
      "iteration 500 / 1000: loss 59.046111\n",
      "iteration 600 / 1000: loss 48.605893\n",
      "iteration 700 / 1000: loss 40.124765\n",
      "iteration 800 / 1000: loss 33.141751\n",
      "iteration 900 / 1000: loss 27.337824\n",
      "(1e-07, 5000.0)\n",
      "(0.28244897959183674, 0.31)\n",
      "\n",
      "iteration 0 / 1000: loss 254.707164\n",
      "iteration 100 / 1000: loss 183.780298\n",
      "iteration 200 / 1000: loss 133.552776\n",
      "iteration 300 / 1000: loss 97.114853\n",
      "iteration 400 / 1000: loss 70.822077\n",
      "iteration 500 / 1000: loss 51.876810\n",
      "iteration 600 / 1000: loss 38.165562\n",
      "iteration 700 / 1000: loss 28.150500\n",
      "iteration 800 / 1000: loss 21.057723\n",
      "iteration 900 / 1000: loss 15.783753\n",
      "(1e-07, 8000.0)\n",
      "(0.3174489795918367, 0.335)\n",
      "\n",
      "iteration 0 / 1000: loss 343.412889\n",
      "iteration 100 / 1000: loss 220.488169\n",
      "iteration 200 / 1000: loss 142.068227\n",
      "iteration 300 / 1000: loss 91.940859\n",
      "iteration 400 / 1000: loss 59.876009\n",
      "iteration 500 / 1000: loss 39.131914\n",
      "iteration 600 / 1000: loss 25.961787\n",
      "iteration 700 / 1000: loss 17.368880\n",
      "iteration 800 / 1000: loss 11.838223\n",
      "iteration 900 / 1000: loss 8.371887\n",
      "(1e-07, 11000.0)\n",
      "(0.33453061224489794, 0.335)\n",
      "\n",
      "iteration 0 / 1000: loss 441.712534\n",
      "iteration 100 / 1000: loss 251.340814\n",
      "iteration 200 / 1000: loss 144.009075\n",
      "iteration 300 / 1000: loss 82.741938\n",
      "iteration 400 / 1000: loss 48.029154\n",
      "iteration 500 / 1000: loss 28.277817\n",
      "iteration 600 / 1000: loss 16.901974\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 700 / 1000: loss 10.512054\n",
      "iteration 800 / 1000: loss 6.894270\n",
      "iteration 900 / 1000: loss 4.816585\n",
      "(1e-07, 14000.0)\n",
      "(0.3382040816326531, 0.355)\n",
      "\n",
      "iteration 0 / 1000: loss 523.328021\n",
      "iteration 100 / 1000: loss 264.557017\n",
      "iteration 200 / 1000: loss 134.530408\n",
      "iteration 300 / 1000: loss 68.933367\n",
      "iteration 400 / 1000: loss 35.751574\n",
      "iteration 500 / 1000: loss 19.137302\n",
      "iteration 600 / 1000: loss 10.672728\n",
      "iteration 700 / 1000: loss 6.471840\n",
      "iteration 800 / 1000: loss 4.244774\n",
      "iteration 900 / 1000: loss 3.173572\n",
      "(1e-07, 17000.0)\n",
      "(0.3369795918367347, 0.35)\n",
      "\n",
      "iteration 0 / 1000: loss 615.664761\n",
      "iteration 100 / 1000: loss 275.772888\n",
      "iteration 200 / 1000: loss 124.357401\n",
      "iteration 300 / 1000: loss 56.855776\n",
      "iteration 400 / 1000: loss 26.616767\n",
      "iteration 500 / 1000: loss 13.023220\n",
      "iteration 600 / 1000: loss 6.918658\n",
      "iteration 700 / 1000: loss 4.246870\n",
      "iteration 800 / 1000: loss 3.043384\n",
      "iteration 900 / 1000: loss 2.520673\n",
      "(1e-07, 20000.0)\n",
      "(0.33938775510204083, 0.345)\n",
      "\n",
      "iteration 0 / 1000: loss 708.025750\n",
      "iteration 100 / 1000: loss 281.553886\n",
      "iteration 200 / 1000: loss 112.892771\n",
      "iteration 300 / 1000: loss 46.033009\n",
      "iteration 400 / 1000: loss 19.463866\n",
      "iteration 500 / 1000: loss 8.955322\n",
      "iteration 600 / 1000: loss 4.841266\n",
      "iteration 700 / 1000: loss 3.177255\n",
      "iteration 800 / 1000: loss 2.517655\n",
      "iteration 900 / 1000: loss 2.260474\n",
      "(1e-07, 23000.0)\n",
      "(0.3310612244897959, 0.343)\n",
      "\n",
      "iteration 0 / 1000: loss 798.859375\n",
      "iteration 100 / 1000: loss 281.686204\n",
      "iteration 200 / 1000: loss 100.464245\n",
      "iteration 300 / 1000: loss 36.663394\n",
      "iteration 400 / 1000: loss 14.250874\n",
      "iteration 500 / 1000: loss 6.380203\n",
      "iteration 600 / 1000: loss 3.620095\n",
      "iteration 700 / 1000: loss 2.621769\n",
      "iteration 800 / 1000: loss 2.285550\n",
      "iteration 900 / 1000: loss 2.096310\n",
      "(1e-07, 26000.0)\n",
      "(0.3254489795918367, 0.339)\n",
      "\n",
      "iteration 0 / 1000: loss 909.082773\n",
      "iteration 100 / 1000: loss 284.572778\n",
      "iteration 200 / 1000: loss 90.059650\n",
      "iteration 300 / 1000: loss 29.524913\n",
      "iteration 400 / 1000: loss 10.625435\n",
      "iteration 500 / 1000: loss 4.744796\n",
      "iteration 600 / 1000: loss 2.958706\n",
      "iteration 700 / 1000: loss 2.327156\n",
      "iteration 800 / 1000: loss 2.151824\n",
      "iteration 900 / 1000: loss 2.151744\n",
      "(1e-07, 29000.0)\n",
      "(0.31885714285714284, 0.331)\n",
      "\n",
      "iteration 0 / 1000: loss 984.539807\n",
      "iteration 100 / 1000: loss 273.026406\n",
      "iteration 200 / 1000: loss 76.783144\n",
      "iteration 300 / 1000: loss 22.761649\n",
      "iteration 400 / 1000: loss 7.803002\n",
      "iteration 500 / 1000: loss 3.646446\n",
      "iteration 600 / 1000: loss 2.542430\n",
      "iteration 700 / 1000: loss 2.204448\n",
      "iteration 800 / 1000: loss 2.136494\n",
      "iteration 900 / 1000: loss 2.123750\n",
      "(1e-07, 32000.0)\n",
      "(0.3203061224489796, 0.332)\n",
      "\n",
      "iteration 0 / 1000: loss 1080.889287\n",
      "iteration 100 / 1000: loss 265.775912\n",
      "iteration 200 / 1000: loss 66.675611\n",
      "iteration 300 / 1000: loss 17.881796\n",
      "iteration 400 / 1000: loss 5.958377\n",
      "iteration 500 / 1000: loss 3.098907\n",
      "iteration 600 / 1000: loss 2.319121\n",
      "iteration 700 / 1000: loss 2.167445\n",
      "iteration 800 / 1000: loss 2.198141\n",
      "iteration 900 / 1000: loss 2.138546\n",
      "(1e-07, 35000.0)\n",
      "(0.3137755102040816, 0.332)\n",
      "\n",
      "iteration 0 / 1000: loss 1174.436356\n",
      "iteration 100 / 1000: loss 256.073720\n",
      "iteration 200 / 1000: loss 57.172177\n",
      "iteration 300 / 1000: loss 14.049784\n",
      "iteration 400 / 1000: loss 4.670178\n",
      "iteration 500 / 1000: loss 2.702044\n",
      "iteration 600 / 1000: loss 2.224499\n",
      "iteration 700 / 1000: loss 2.196853\n",
      "iteration 800 / 1000: loss 2.125486\n",
      "iteration 900 / 1000: loss 2.131826\n",
      "(1e-07, 38000.0)\n",
      "(0.3222244897959184, 0.342)\n",
      "\n",
      "iteration 0 / 1000: loss 1275.398574\n",
      "iteration 100 / 1000: loss 246.658288\n",
      "iteration 200 / 1000: loss 49.065066\n",
      "iteration 300 / 1000: loss 11.210673\n",
      "iteration 400 / 1000: loss 3.919729\n",
      "iteration 500 / 1000: loss 2.428838\n",
      "iteration 600 / 1000: loss 2.190323\n",
      "iteration 700 / 1000: loss 2.177200\n",
      "iteration 800 / 1000: loss 2.127648\n",
      "iteration 900 / 1000: loss 2.154331\n",
      "(1e-07, 41000.0)\n",
      "(0.3176122448979592, 0.334)\n",
      "\n",
      "iteration 0 / 1000: loss 1358.276866\n",
      "iteration 100 / 1000: loss 232.781249\n",
      "iteration 200 / 1000: loss 41.383215\n",
      "iteration 300 / 1000: loss 8.828507\n",
      "iteration 400 / 1000: loss 3.303283\n",
      "iteration 500 / 1000: loss 2.327189\n",
      "iteration 600 / 1000: loss 2.113876\n",
      "iteration 700 / 1000: loss 2.107703\n",
      "iteration 800 / 1000: loss 2.103816\n",
      "iteration 900 / 1000: loss 2.167307\n",
      "(1e-07, 44000.0)\n",
      "(0.3122244897959184, 0.321)\n",
      "\n",
      "iteration 0 / 1000: loss 1445.698771\n",
      "iteration 100 / 1000: loss 219.357114\n",
      "iteration 200 / 1000: loss 34.862859\n",
      "iteration 300 / 1000: loss 7.069419\n",
      "iteration 400 / 1000: loss 2.832022\n",
      "iteration 500 / 1000: loss 2.255692\n",
      "iteration 600 / 1000: loss 2.150522\n",
      "iteration 700 / 1000: loss 2.131836\n",
      "iteration 800 / 1000: loss 2.171826\n",
      "iteration 900 / 1000: loss 2.157834\n",
      "(1e-07, 47000.0)\n",
      "(0.30920408163265306, 0.328)\n",
      "\n",
      "iteration 0 / 1000: loss 1528.060136\n",
      "iteration 100 / 1000: loss 205.736872\n",
      "iteration 200 / 1000: loss 29.329576\n",
      "iteration 300 / 1000: loss 5.772541\n",
      "iteration 400 / 1000: loss 2.603894\n",
      "iteration 500 / 1000: loss 2.259926\n",
      "iteration 600 / 1000: loss 2.151268\n",
      "iteration 700 / 1000: loss 2.182723\n",
      "iteration 800 / 1000: loss 2.134636\n",
      "iteration 900 / 1000: loss 2.139552\n",
      "(1e-07, 50000.0)\n",
      "(0.30714285714285716, 0.326)\n",
      "\n",
      "iteration 0 / 1000: loss 160.072623\n",
      "iteration 100 / 1000: loss 105.784076\n",
      "iteration 200 / 1000: loss 71.186252\n",
      "iteration 300 / 1000: loss 48.326081\n",
      "iteration 400 / 1000: loss 32.892673\n",
      "iteration 500 / 1000: loss 22.729479\n",
      "iteration 600 / 1000: loss 15.672169\n",
      "iteration 700 / 1000: loss 11.049121\n",
      "iteration 800 / 1000: loss 8.036947\n",
      "iteration 900 / 1000: loss 6.011440\n",
      "(2e-07, 5000.0)\n",
      "(0.35712244897959183, 0.371)\n",
      "\n",
      "iteration 0 / 1000: loss 250.357214\n",
      "iteration 100 / 1000: loss 130.924712\n",
      "iteration 200 / 1000: loss 69.713824\n",
      "iteration 300 / 1000: loss 37.568860\n",
      "iteration 400 / 1000: loss 20.546424\n",
      "iteration 500 / 1000: loss 11.781272\n",
      "iteration 600 / 1000: loss 7.122096\n",
      "iteration 700 / 1000: loss 4.705376\n",
      "iteration 800 / 1000: loss 3.404159\n",
      "iteration 900 / 1000: loss 2.731759\n",
      "(2e-07, 8000.0)\n",
      "(0.3608571428571429, 0.375)\n",
      "\n",
      "iteration 0 / 1000: loss 346.057330\n",
      "iteration 100 / 1000: loss 143.435916\n",
      "iteration 200 / 1000: loss 60.221477\n",
      "iteration 300 / 1000: loss 25.962365\n",
      "iteration 400 / 1000: loss 11.861423\n",
      "iteration 500 / 1000: loss 5.976269\n",
      "iteration 600 / 1000: loss 3.651998\n",
      "iteration 700 / 1000: loss 2.747212\n",
      "iteration 800 / 1000: loss 2.308061\n",
      "iteration 900 / 1000: loss 2.133173\n",
      "(2e-07, 11000.0)\n",
      "(0.3487959183673469, 0.367)\n",
      "\n",
      "iteration 0 / 1000: loss 433.771460\n",
      "iteration 100 / 1000: loss 141.036669\n",
      "iteration 200 / 1000: loss 47.071148\n",
      "iteration 300 / 1000: loss 16.563795\n",
      "iteration 400 / 1000: loss 6.723314\n",
      "iteration 500 / 1000: loss 3.603204\n",
      "iteration 600 / 1000: loss 2.527469\n",
      "iteration 700 / 1000: loss 2.110963\n",
      "iteration 800 / 1000: loss 2.070500\n",
      "iteration 900 / 1000: loss 2.058133\n",
      "(2e-07, 14000.0)\n",
      "(0.34485714285714286, 0.365)\n",
      "\n",
      "iteration 0 / 1000: loss 527.356108\n",
      "iteration 100 / 1000: loss 135.219454\n",
      "iteration 200 / 1000: loss 35.858858\n",
      "iteration 300 / 1000: loss 10.685153\n",
      "iteration 400 / 1000: loss 4.187576\n",
      "iteration 500 / 1000: loss 2.598279\n",
      "iteration 600 / 1000: loss 2.256960\n",
      "iteration 700 / 1000: loss 2.072375\n",
      "iteration 800 / 1000: loss 2.097166\n",
      "iteration 900 / 1000: loss 1.978149\n",
      "(2e-07, 17000.0)\n",
      "(0.3349591836734694, 0.349)\n",
      "\n",
      "iteration 0 / 1000: loss 614.941504\n",
      "iteration 100 / 1000: loss 123.794834\n",
      "iteration 200 / 1000: loss 26.303070\n",
      "iteration 300 / 1000: loss 6.932400\n",
      "iteration 400 / 1000: loss 3.007966\n",
      "iteration 500 / 1000: loss 2.253278\n",
      "iteration 600 / 1000: loss 2.154899\n",
      "iteration 700 / 1000: loss 2.077995\n",
      "iteration 800 / 1000: loss 2.044376\n",
      "iteration 900 / 1000: loss 1.994391\n",
      "(2e-07, 20000.0)\n",
      "(0.33646938775510205, 0.353)\n",
      "\n",
      "iteration 0 / 1000: loss 708.788310\n",
      "iteration 100 / 1000: loss 112.696411\n",
      "iteration 200 / 1000: loss 19.367988\n",
      "iteration 300 / 1000: loss 4.798829\n",
      "iteration 400 / 1000: loss 2.479362\n",
      "iteration 500 / 1000: loss 2.127464\n",
      "iteration 600 / 1000: loss 2.112081\n",
      "iteration 700 / 1000: loss 2.117027\n",
      "iteration 800 / 1000: loss 2.084123\n",
      "iteration 900 / 1000: loss 2.111188\n",
      "(2e-07, 23000.0)\n",
      "(0.33055102040816325, 0.343)\n",
      "\n",
      "iteration 0 / 1000: loss 794.911698\n",
      "iteration 100 / 1000: loss 99.420625\n",
      "iteration 200 / 1000: loss 14.086610\n",
      "iteration 300 / 1000: loss 3.549703\n",
      "iteration 400 / 1000: loss 2.304961\n",
      "iteration 500 / 1000: loss 2.139871\n",
      "iteration 600 / 1000: loss 2.087703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 700 / 1000: loss 2.068973\n",
      "iteration 800 / 1000: loss 2.112945\n",
      "iteration 900 / 1000: loss 2.072117\n",
      "(2e-07, 26000.0)\n",
      "(0.3274489795918367, 0.338)\n",
      "\n",
      "iteration 0 / 1000: loss 907.451160\n",
      "iteration 100 / 1000: loss 89.329452\n",
      "iteration 200 / 1000: loss 10.499370\n",
      "iteration 300 / 1000: loss 2.998997\n",
      "iteration 400 / 1000: loss 2.172867\n",
      "iteration 500 / 1000: loss 2.094107\n",
      "iteration 600 / 1000: loss 2.136484\n",
      "iteration 700 / 1000: loss 2.029752\n",
      "iteration 800 / 1000: loss 2.127259\n",
      "iteration 900 / 1000: loss 2.060267\n",
      "(2e-07, 29000.0)\n",
      "(0.32240816326530614, 0.347)\n",
      "\n",
      "iteration 0 / 1000: loss 980.283475\n",
      "iteration 100 / 1000: loss 75.923890\n",
      "iteration 200 / 1000: loss 7.710179\n",
      "iteration 300 / 1000: loss 2.555001\n",
      "iteration 400 / 1000: loss 2.120237\n",
      "iteration 500 / 1000: loss 2.082044\n",
      "iteration 600 / 1000: loss 2.108056\n",
      "iteration 700 / 1000: loss 2.112979\n",
      "iteration 800 / 1000: loss 2.136107\n",
      "iteration 900 / 1000: loss 2.138812\n",
      "(2e-07, 32000.0)\n",
      "(0.3263265306122449, 0.335)\n",
      "\n",
      "iteration 0 / 1000: loss 1073.780379\n",
      "iteration 100 / 1000: loss 65.670466\n",
      "iteration 200 / 1000: loss 5.827443\n",
      "iteration 300 / 1000: loss 2.340291\n",
      "iteration 400 / 1000: loss 2.135068\n",
      "iteration 500 / 1000: loss 2.097743\n",
      "iteration 600 / 1000: loss 2.053475\n",
      "iteration 700 / 1000: loss 2.117754\n",
      "iteration 800 / 1000: loss 2.115588\n",
      "iteration 900 / 1000: loss 2.150762\n",
      "(2e-07, 35000.0)\n",
      "(0.3143061224489796, 0.33)\n",
      "\n",
      "iteration 0 / 1000: loss 1173.717897\n",
      "iteration 100 / 1000: loss 56.409971\n",
      "iteration 200 / 1000: loss 4.661074\n",
      "iteration 300 / 1000: loss 2.282198\n",
      "iteration 400 / 1000: loss 2.133821\n",
      "iteration 500 / 1000: loss 2.084810\n",
      "iteration 600 / 1000: loss 2.105153\n",
      "iteration 700 / 1000: loss 2.058468\n",
      "iteration 800 / 1000: loss 2.143434\n",
      "iteration 900 / 1000: loss 2.123875\n",
      "(2e-07, 38000.0)\n",
      "(0.3129183673469388, 0.328)\n",
      "\n",
      "iteration 0 / 1000: loss 1290.930221\n",
      "iteration 100 / 1000: loss 48.910486\n",
      "iteration 200 / 1000: loss 3.834791\n",
      "iteration 300 / 1000: loss 2.184690\n",
      "iteration 400 / 1000: loss 2.104965\n",
      "iteration 500 / 1000: loss 2.117404\n",
      "iteration 600 / 1000: loss 2.152291\n",
      "iteration 700 / 1000: loss 2.109745\n",
      "iteration 800 / 1000: loss 2.107606\n",
      "iteration 900 / 1000: loss 2.105141\n",
      "(2e-07, 41000.0)\n",
      "(0.31642857142857145, 0.333)\n",
      "\n",
      "iteration 0 / 1000: loss 1367.113106\n",
      "iteration 100 / 1000: loss 40.995144\n",
      "iteration 200 / 1000: loss 3.230064\n",
      "iteration 300 / 1000: loss 2.178211\n",
      "iteration 400 / 1000: loss 2.208279\n",
      "iteration 500 / 1000: loss 2.149717\n",
      "iteration 600 / 1000: loss 2.141866\n",
      "iteration 700 / 1000: loss 2.133282\n",
      "iteration 800 / 1000: loss 2.116573\n",
      "iteration 900 / 1000: loss 2.130005\n",
      "(2e-07, 44000.0)\n",
      "(0.3042448979591837, 0.325)\n",
      "\n",
      "iteration 0 / 1000: loss 1438.080285\n",
      "iteration 100 / 1000: loss 34.157758\n",
      "iteration 200 / 1000: loss 2.856282\n",
      "iteration 300 / 1000: loss 2.211239\n",
      "iteration 400 / 1000: loss 2.129552\n",
      "iteration 500 / 1000: loss 2.152543\n",
      "iteration 600 / 1000: loss 2.112285\n",
      "iteration 700 / 1000: loss 2.169069\n",
      "iteration 800 / 1000: loss 2.130327\n",
      "iteration 900 / 1000: loss 2.113642\n",
      "(2e-07, 47000.0)\n",
      "(0.2963265306122449, 0.311)\n",
      "\n",
      "iteration 0 / 1000: loss 1555.799885\n",
      "iteration 100 / 1000: loss 29.267835\n",
      "iteration 200 / 1000: loss 2.600090\n",
      "iteration 300 / 1000: loss 2.164607\n",
      "iteration 400 / 1000: loss 2.123049\n",
      "iteration 500 / 1000: loss 2.161269\n",
      "iteration 600 / 1000: loss 2.098359\n",
      "iteration 700 / 1000: loss 2.137968\n",
      "iteration 800 / 1000: loss 2.183151\n",
      "iteration 900 / 1000: loss 2.157679\n",
      "(2e-07, 50000.0)\n",
      "(0.31057142857142855, 0.322)\n",
      "\n",
      "iteration 0 / 1000: loss 159.755777\n",
      "iteration 100 / 1000: loss 58.556486\n",
      "iteration 200 / 1000: loss 22.597089\n",
      "iteration 300 / 1000: loss 9.458094\n",
      "iteration 400 / 1000: loss 4.713301\n",
      "iteration 500 / 1000: loss 2.901728\n",
      "iteration 600 / 1000: loss 2.256894\n",
      "iteration 700 / 1000: loss 2.121529\n",
      "iteration 800 / 1000: loss 2.057027\n",
      "iteration 900 / 1000: loss 1.979361\n",
      "(5e-07, 5000.0)\n",
      "(0.37412244897959185, 0.381)\n",
      "\n",
      "iteration 0 / 1000: loss 253.329030\n",
      "iteration 100 / 1000: loss 51.261824\n",
      "iteration 200 / 1000: loss 11.816792\n",
      "iteration 300 / 1000: loss 3.950353\n",
      "iteration 400 / 1000: loss 2.359194\n",
      "iteration 500 / 1000: loss 2.086445\n",
      "iteration 600 / 1000: loss 2.104078\n",
      "iteration 700 / 1000: loss 1.961229\n",
      "iteration 800 / 1000: loss 1.975094\n",
      "iteration 900 / 1000: loss 2.008452\n",
      "(5e-07, 8000.0)\n",
      "(0.3606530612244898, 0.377)\n",
      "\n",
      "iteration 0 / 1000: loss 345.079823\n",
      "iteration 100 / 1000: loss 38.844239\n",
      "iteration 200 / 1000: loss 6.085427\n",
      "iteration 300 / 1000: loss 2.439391\n",
      "iteration 400 / 1000: loss 2.051459\n",
      "iteration 500 / 1000: loss 2.016370\n",
      "iteration 600 / 1000: loss 2.043827\n",
      "iteration 700 / 1000: loss 2.082251\n",
      "iteration 800 / 1000: loss 1.948295\n",
      "iteration 900 / 1000: loss 2.001307\n",
      "(5e-07, 11000.0)\n",
      "(0.35244897959183674, 0.362)\n",
      "\n",
      "iteration 0 / 1000: loss 437.063188\n",
      "iteration 100 / 1000: loss 27.462688\n",
      "iteration 200 / 1000: loss 3.575820\n",
      "iteration 300 / 1000: loss 2.135148\n",
      "iteration 400 / 1000: loss 2.026176\n",
      "iteration 500 / 1000: loss 1.967731\n",
      "iteration 600 / 1000: loss 2.026432\n",
      "iteration 700 / 1000: loss 1.992898\n",
      "iteration 800 / 1000: loss 2.115371\n",
      "iteration 900 / 1000: loss 2.073881\n",
      "(5e-07, 14000.0)\n",
      "(0.34010204081632656, 0.351)\n",
      "\n",
      "iteration 0 / 1000: loss 533.725662\n",
      "iteration 100 / 1000: loss 18.985079\n",
      "iteration 200 / 1000: loss 2.548882\n",
      "iteration 300 / 1000: loss 2.106332\n",
      "iteration 400 / 1000: loss 2.076990\n",
      "iteration 500 / 1000: loss 2.052908\n",
      "iteration 600 / 1000: loss 2.058201\n",
      "iteration 700 / 1000: loss 2.051799\n",
      "iteration 800 / 1000: loss 2.046723\n",
      "iteration 900 / 1000: loss 2.069680\n",
      "(5e-07, 17000.0)\n",
      "(0.33591836734693875, 0.35)\n",
      "\n",
      "iteration 0 / 1000: loss 620.442415\n",
      "iteration 100 / 1000: loss 12.869243\n",
      "iteration 200 / 1000: loss 2.206518\n",
      "iteration 300 / 1000: loss 2.119588\n",
      "iteration 400 / 1000: loss 2.081134\n",
      "iteration 500 / 1000: loss 2.101477\n",
      "iteration 600 / 1000: loss 2.079692\n",
      "iteration 700 / 1000: loss 2.068152\n",
      "iteration 800 / 1000: loss 2.043180\n",
      "iteration 900 / 1000: loss 2.070132\n",
      "(5e-07, 20000.0)\n",
      "(0.3296938775510204, 0.346)\n",
      "\n",
      "iteration 0 / 1000: loss 707.459214\n",
      "iteration 100 / 1000: loss 8.708459\n",
      "iteration 200 / 1000: loss 2.164874\n",
      "iteration 300 / 1000: loss 2.067506\n",
      "iteration 400 / 1000: loss 2.082106\n",
      "iteration 500 / 1000: loss 2.092555\n",
      "iteration 600 / 1000: loss 2.031684\n",
      "iteration 700 / 1000: loss 2.124784\n",
      "iteration 800 / 1000: loss 2.052506\n",
      "iteration 900 / 1000: loss 2.098308\n",
      "(5e-07, 23000.0)\n",
      "(0.33587755102040817, 0.337)\n",
      "\n",
      "iteration 0 / 1000: loss 809.677979\n",
      "iteration 100 / 1000: loss 6.189161\n",
      "iteration 200 / 1000: loss 2.150034\n",
      "iteration 300 / 1000: loss 2.083402\n",
      "iteration 400 / 1000: loss 2.068293\n",
      "iteration 500 / 1000: loss 2.063340\n",
      "iteration 600 / 1000: loss 2.111160\n",
      "iteration 700 / 1000: loss 2.131893\n",
      "iteration 800 / 1000: loss 2.088071\n",
      "iteration 900 / 1000: loss 2.165962\n",
      "(5e-07, 26000.0)\n",
      "(0.3323265306122449, 0.354)\n",
      "\n",
      "iteration 0 / 1000: loss 902.866614\n",
      "iteration 100 / 1000: loss 4.624815\n",
      "iteration 200 / 1000: loss 2.196629\n",
      "iteration 300 / 1000: loss 2.076567\n",
      "iteration 400 / 1000: loss 2.085925\n",
      "iteration 500 / 1000: loss 2.113483\n",
      "iteration 600 / 1000: loss 2.089724\n",
      "iteration 700 / 1000: loss 2.101476\n",
      "iteration 800 / 1000: loss 2.104577\n",
      "iteration 900 / 1000: loss 2.096262\n",
      "(5e-07, 29000.0)\n",
      "(0.31526530612244896, 0.333)\n",
      "\n",
      "iteration 0 / 1000: loss 1008.145447\n",
      "iteration 100 / 1000: loss 3.580119\n",
      "iteration 200 / 1000: loss 2.129140\n",
      "iteration 300 / 1000: loss 2.169940\n",
      "iteration 400 / 1000: loss 2.103270\n",
      "iteration 500 / 1000: loss 2.081584\n",
      "iteration 600 / 1000: loss 2.097103\n",
      "iteration 700 / 1000: loss 2.131375\n",
      "iteration 800 / 1000: loss 2.143366\n",
      "iteration 900 / 1000: loss 2.109236\n",
      "(5e-07, 32000.0)\n",
      "(0.3142857142857143, 0.332)\n",
      "\n",
      "iteration 0 / 1000: loss 1093.713508\n",
      "iteration 100 / 1000: loss 2.970096\n",
      "iteration 200 / 1000: loss 2.088985\n",
      "iteration 300 / 1000: loss 2.117402\n",
      "iteration 400 / 1000: loss 2.136238\n",
      "iteration 500 / 1000: loss 2.130188\n",
      "iteration 600 / 1000: loss 2.150585\n",
      "iteration 700 / 1000: loss 2.177540\n",
      "iteration 800 / 1000: loss 2.108552\n",
      "iteration 900 / 1000: loss 2.119992\n",
      "(5e-07, 35000.0)\n",
      "(0.307, 0.327)\n",
      "\n",
      "iteration 0 / 1000: loss 1148.899859\n",
      "iteration 100 / 1000: loss 2.607153\n",
      "iteration 200 / 1000: loss 2.077260\n",
      "iteration 300 / 1000: loss 2.110141\n",
      "iteration 400 / 1000: loss 2.144389\n",
      "iteration 500 / 1000: loss 2.103866\n",
      "iteration 600 / 1000: loss 2.130136\n",
      "iteration 700 / 1000: loss 2.133962\n",
      "iteration 800 / 1000: loss 2.120970\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 900 / 1000: loss 2.091745\n",
      "(5e-07, 38000.0)\n",
      "(0.31651020408163266, 0.333)\n",
      "\n",
      "iteration 0 / 1000: loss 1279.229155\n",
      "iteration 100 / 1000: loss 2.452970\n",
      "iteration 200 / 1000: loss 2.143342\n",
      "iteration 300 / 1000: loss 2.114705\n",
      "iteration 400 / 1000: loss 2.173386\n",
      "iteration 500 / 1000: loss 2.145592\n",
      "iteration 600 / 1000: loss 2.147592\n",
      "iteration 700 / 1000: loss 2.096870\n",
      "iteration 800 / 1000: loss 2.101478\n",
      "iteration 900 / 1000: loss 2.074701\n",
      "(5e-07, 41000.0)\n",
      "(0.3066122448979592, 0.32)\n",
      "\n",
      "iteration 0 / 1000: loss 1354.062627\n",
      "iteration 100 / 1000: loss 2.324362\n",
      "iteration 200 / 1000: loss 2.178107\n",
      "iteration 300 / 1000: loss 2.182024\n",
      "iteration 400 / 1000: loss 2.134334\n",
      "iteration 500 / 1000: loss 2.113910\n",
      "iteration 600 / 1000: loss 2.177126\n",
      "iteration 700 / 1000: loss 2.169983\n",
      "iteration 800 / 1000: loss 2.157592\n",
      "iteration 900 / 1000: loss 2.135255\n",
      "(5e-07, 44000.0)\n",
      "(0.3090204081632653, 0.331)\n",
      "\n",
      "iteration 0 / 1000: loss 1431.017037\n",
      "iteration 100 / 1000: loss 2.221983\n",
      "iteration 200 / 1000: loss 2.076627\n",
      "iteration 300 / 1000: loss 2.182764\n",
      "iteration 400 / 1000: loss 2.140693\n",
      "iteration 500 / 1000: loss 2.120175\n",
      "iteration 600 / 1000: loss 2.177058\n",
      "iteration 700 / 1000: loss 2.153616\n",
      "iteration 800 / 1000: loss 2.159450\n",
      "iteration 900 / 1000: loss 2.117548\n",
      "(5e-07, 47000.0)\n",
      "(0.3166938775510204, 0.332)\n",
      "\n",
      "iteration 0 / 1000: loss 1549.076060\n",
      "iteration 100 / 1000: loss 2.208569\n",
      "iteration 200 / 1000: loss 2.204388\n",
      "iteration 300 / 1000: loss 2.189966\n",
      "iteration 400 / 1000: loss 2.144923\n",
      "iteration 500 / 1000: loss 2.168538\n",
      "iteration 600 / 1000: loss 2.133425\n",
      "iteration 700 / 1000: loss 2.167771\n",
      "iteration 800 / 1000: loss 2.148554\n",
      "iteration 900 / 1000: loss 2.138860\n",
      "(5e-07, 50000.0)\n",
      "(0.305, 0.316)\n",
      "\n",
      "iteration 0 / 1000: loss 157.628585\n",
      "iteration 100 / 1000: loss 22.125369\n",
      "iteration 200 / 1000: loss 4.633365\n",
      "iteration 300 / 1000: loss 2.351460\n",
      "iteration 400 / 1000: loss 2.001275\n",
      "iteration 500 / 1000: loss 1.978236\n",
      "iteration 600 / 1000: loss 1.939864\n",
      "iteration 700 / 1000: loss 1.967531\n",
      "iteration 800 / 1000: loss 1.938376\n",
      "iteration 900 / 1000: loss 2.014967\n",
      "(1e-06, 5000.0)\n",
      "(0.3686734693877551, 0.389)\n",
      "\n",
      "iteration 0 / 1000: loss 247.048341\n",
      "iteration 100 / 1000: loss 11.421087\n",
      "iteration 200 / 1000: loss 2.318020\n",
      "iteration 300 / 1000: loss 2.014126\n",
      "iteration 400 / 1000: loss 1.958307\n",
      "iteration 500 / 1000: loss 2.100968\n",
      "iteration 600 / 1000: loss 1.957195\n",
      "iteration 700 / 1000: loss 1.999793\n",
      "iteration 800 / 1000: loss 2.031612\n",
      "iteration 900 / 1000: loss 2.023587\n",
      "(1e-06, 8000.0)\n",
      "(0.35422448979591836, 0.363)\n",
      "\n",
      "iteration 0 / 1000: loss 347.321568\n",
      "iteration 100 / 1000: loss 6.018933\n",
      "iteration 200 / 1000: loss 2.081222\n",
      "iteration 300 / 1000: loss 1.995951\n",
      "iteration 400 / 1000: loss 2.041834\n",
      "iteration 500 / 1000: loss 2.058401\n",
      "iteration 600 / 1000: loss 2.043469\n",
      "iteration 700 / 1000: loss 1.928306\n",
      "iteration 800 / 1000: loss 2.007425\n",
      "iteration 900 / 1000: loss 2.017111\n",
      "(1e-06, 11000.0)\n",
      "(0.3448163265306122, 0.35)\n",
      "\n",
      "iteration 0 / 1000: loss 429.679800\n",
      "iteration 100 / 1000: loss 3.395574\n",
      "iteration 200 / 1000: loss 2.046735\n",
      "iteration 300 / 1000: loss 2.059038\n",
      "iteration 400 / 1000: loss 2.085837\n",
      "iteration 500 / 1000: loss 2.068480\n",
      "iteration 600 / 1000: loss 2.005183\n",
      "iteration 700 / 1000: loss 2.060396\n",
      "iteration 800 / 1000: loss 2.046776\n",
      "iteration 900 / 1000: loss 2.031226\n",
      "(1e-06, 14000.0)\n",
      "(0.3402857142857143, 0.349)\n",
      "\n",
      "iteration 0 / 1000: loss 529.966576\n",
      "iteration 100 / 1000: loss 2.522477\n",
      "iteration 200 / 1000: loss 2.079678\n",
      "iteration 300 / 1000: loss 2.079520\n",
      "iteration 400 / 1000: loss 2.026265\n",
      "iteration 500 / 1000: loss 2.098239\n",
      "iteration 600 / 1000: loss 2.063414\n",
      "iteration 700 / 1000: loss 2.039573\n",
      "iteration 800 / 1000: loss 2.061917\n",
      "iteration 900 / 1000: loss 2.044995\n",
      "(1e-06, 17000.0)\n",
      "(0.33848979591836736, 0.367)\n",
      "\n",
      "iteration 0 / 1000: loss 616.093960\n",
      "iteration 100 / 1000: loss 2.214293\n",
      "iteration 200 / 1000: loss 2.135857\n",
      "iteration 300 / 1000: loss 2.078371\n",
      "iteration 400 / 1000: loss 2.066593\n",
      "iteration 500 / 1000: loss 2.063795\n",
      "iteration 600 / 1000: loss 2.082294\n",
      "iteration 700 / 1000: loss 2.026786\n",
      "iteration 800 / 1000: loss 2.091278\n",
      "iteration 900 / 1000: loss 2.044034\n",
      "(1e-06, 20000.0)\n",
      "(0.3211428571428571, 0.327)\n",
      "\n",
      "iteration 0 / 1000: loss 709.779623\n",
      "iteration 100 / 1000: loss 2.140491\n",
      "iteration 200 / 1000: loss 2.049210\n",
      "iteration 300 / 1000: loss 2.044821\n",
      "iteration 400 / 1000: loss 2.072599\n",
      "iteration 500 / 1000: loss 2.078842\n",
      "iteration 600 / 1000: loss 2.019290\n",
      "iteration 700 / 1000: loss 2.015881\n",
      "iteration 800 / 1000: loss 2.109090\n",
      "iteration 900 / 1000: loss 2.135853\n",
      "(1e-06, 23000.0)\n",
      "(0.3178367346938776, 0.328)\n",
      "\n",
      "iteration 0 / 1000: loss 803.438462\n",
      "iteration 100 / 1000: loss 2.163290\n",
      "iteration 200 / 1000: loss 2.081605\n",
      "iteration 300 / 1000: loss 2.056348\n",
      "iteration 400 / 1000: loss 2.135709\n",
      "iteration 500 / 1000: loss 2.060746\n",
      "iteration 600 / 1000: loss 2.105979\n",
      "iteration 700 / 1000: loss 2.110289\n",
      "iteration 800 / 1000: loss 2.059326\n",
      "iteration 900 / 1000: loss 2.082679\n",
      "(1e-06, 26000.0)\n",
      "(0.30948979591836734, 0.313)\n",
      "\n",
      "iteration 0 / 1000: loss 896.104588\n",
      "iteration 100 / 1000: loss 2.119082\n",
      "iteration 200 / 1000: loss 2.093430\n",
      "iteration 300 / 1000: loss 2.152064\n",
      "iteration 400 / 1000: loss 2.092734\n",
      "iteration 500 / 1000: loss 2.133909\n",
      "iteration 600 / 1000: loss 2.089652\n",
      "iteration 700 / 1000: loss 2.077327\n",
      "iteration 800 / 1000: loss 2.131078\n",
      "iteration 900 / 1000: loss 2.125475\n",
      "(1e-06, 29000.0)\n",
      "(0.2998571428571429, 0.329)\n",
      "\n",
      "iteration 0 / 1000: loss 988.802740\n",
      "iteration 100 / 1000: loss 2.236821\n",
      "iteration 200 / 1000: loss 2.120223\n",
      "iteration 300 / 1000: loss 2.056013\n",
      "iteration 400 / 1000: loss 2.104404\n",
      "iteration 500 / 1000: loss 2.132827\n",
      "iteration 600 / 1000: loss 2.026080\n",
      "iteration 700 / 1000: loss 2.131430\n",
      "iteration 800 / 1000: loss 2.060715\n",
      "iteration 900 / 1000: loss 2.156184\n",
      "(1e-06, 32000.0)\n",
      "(0.306, 0.32)\n",
      "\n",
      "iteration 0 / 1000: loss 1078.244613\n",
      "iteration 100 / 1000: loss 2.135412\n",
      "iteration 200 / 1000: loss 2.085405\n",
      "iteration 300 / 1000: loss 2.069458\n",
      "iteration 400 / 1000: loss 2.100356\n",
      "iteration 500 / 1000: loss 2.109555\n",
      "iteration 600 / 1000: loss 2.125618\n",
      "iteration 700 / 1000: loss 2.143971\n",
      "iteration 800 / 1000: loss 2.134876\n",
      "iteration 900 / 1000: loss 2.146875\n",
      "(1e-06, 35000.0)\n",
      "(0.3058571428571429, 0.307)\n",
      "\n",
      "iteration 0 / 1000: loss 1189.865635\n",
      "iteration 100 / 1000: loss 2.135330\n",
      "iteration 200 / 1000: loss 2.123122\n",
      "iteration 300 / 1000: loss 2.155466\n",
      "iteration 400 / 1000: loss 2.127871\n",
      "iteration 500 / 1000: loss 2.072779\n",
      "iteration 600 / 1000: loss 2.150728\n",
      "iteration 700 / 1000: loss 2.109673\n",
      "iteration 800 / 1000: loss 2.191756\n",
      "iteration 900 / 1000: loss 2.136853\n",
      "(1e-06, 38000.0)\n",
      "(0.29591836734693877, 0.306)\n",
      "\n",
      "iteration 0 / 1000: loss 1265.576237\n",
      "iteration 100 / 1000: loss 2.189850\n",
      "iteration 200 / 1000: loss 2.125875\n",
      "iteration 300 / 1000: loss 2.154160\n",
      "iteration 400 / 1000: loss 2.100155\n",
      "iteration 500 / 1000: loss 2.152110\n",
      "iteration 600 / 1000: loss 2.141487\n",
      "iteration 700 / 1000: loss 2.136732\n",
      "iteration 800 / 1000: loss 2.105339\n",
      "iteration 900 / 1000: loss 2.148004\n",
      "(1e-06, 41000.0)\n",
      "(0.2993265306122449, 0.315)\n",
      "\n",
      "iteration 0 / 1000: loss 1362.316291\n",
      "iteration 100 / 1000: loss 2.144636\n",
      "iteration 200 / 1000: loss 2.119507\n",
      "iteration 300 / 1000: loss 2.131480\n",
      "iteration 400 / 1000: loss 2.152341\n",
      "iteration 500 / 1000: loss 2.210915\n",
      "iteration 600 / 1000: loss 2.123814\n",
      "iteration 700 / 1000: loss 2.141225\n",
      "iteration 800 / 1000: loss 2.131212\n",
      "iteration 900 / 1000: loss 2.207908\n",
      "(1e-06, 44000.0)\n",
      "(0.306, 0.315)\n",
      "\n",
      "iteration 0 / 1000: loss 1444.850249\n",
      "iteration 100 / 1000: loss 2.101243\n",
      "iteration 200 / 1000: loss 2.128271\n",
      "iteration 300 / 1000: loss 2.132749\n",
      "iteration 400 / 1000: loss 2.120657\n",
      "iteration 500 / 1000: loss 2.163568\n",
      "iteration 600 / 1000: loss 2.174922\n",
      "iteration 700 / 1000: loss 2.147021\n",
      "iteration 800 / 1000: loss 2.178804\n",
      "iteration 900 / 1000: loss 2.139957\n",
      "(1e-06, 47000.0)\n",
      "(0.3122448979591837, 0.33)\n",
      "\n",
      "iteration 0 / 1000: loss 1545.520064\n",
      "iteration 100 / 1000: loss 2.200928\n",
      "iteration 200 / 1000: loss 2.223365\n",
      "iteration 300 / 1000: loss 2.111196\n",
      "iteration 400 / 1000: loss 2.139116\n",
      "iteration 500 / 1000: loss 2.128138\n",
      "iteration 600 / 1000: loss 2.179372\n",
      "iteration 700 / 1000: loss 2.132275\n",
      "iteration 800 / 1000: loss 2.151068\n",
      "iteration 900 / 1000: loss 2.149862\n",
      "(1e-06, 50000.0)\n",
      "(0.28922448979591836, 0.292)\n",
      "\n",
      "lr 5.000000e-08 reg 5.000000e+03 train accuracy: 0.218061 val accuracy: 0.253000\n",
      "lr 5.000000e-08 reg 8.000000e+03 train accuracy: 0.250184 val accuracy: 0.256000\n",
      "lr 5.000000e-08 reg 1.100000e+04 train accuracy: 0.259327 val accuracy: 0.278000\n",
      "lr 5.000000e-08 reg 1.400000e+04 train accuracy: 0.269857 val accuracy: 0.275000\n",
      "lr 5.000000e-08 reg 1.700000e+04 train accuracy: 0.291857 val accuracy: 0.298000\n",
      "lr 5.000000e-08 reg 2.000000e+04 train accuracy: 0.301776 val accuracy: 0.319000\n",
      "lr 5.000000e-08 reg 2.300000e+04 train accuracy: 0.297735 val accuracy: 0.301000\n",
      "lr 5.000000e-08 reg 2.600000e+04 train accuracy: 0.309816 val accuracy: 0.320000\n",
      "lr 5.000000e-08 reg 2.900000e+04 train accuracy: 0.309510 val accuracy: 0.311000\n",
      "lr 5.000000e-08 reg 3.200000e+04 train accuracy: 0.311510 val accuracy: 0.313000\n",
      "lr 5.000000e-08 reg 3.500000e+04 train accuracy: 0.317265 val accuracy: 0.333000\n",
      "lr 5.000000e-08 reg 3.800000e+04 train accuracy: 0.315347 val accuracy: 0.328000\n",
      "lr 5.000000e-08 reg 4.100000e+04 train accuracy: 0.312959 val accuracy: 0.325000\n",
      "lr 5.000000e-08 reg 4.400000e+04 train accuracy: 0.310510 val accuracy: 0.323000\n",
      "lr 5.000000e-08 reg 4.700000e+04 train accuracy: 0.313265 val accuracy: 0.325000\n",
      "lr 5.000000e-08 reg 5.000000e+04 train accuracy: 0.304796 val accuracy: 0.327000\n",
      "lr 1.000000e-07 reg 5.000000e+03 train accuracy: 0.282449 val accuracy: 0.310000\n",
      "lr 1.000000e-07 reg 8.000000e+03 train accuracy: 0.317449 val accuracy: 0.335000\n",
      "lr 1.000000e-07 reg 1.100000e+04 train accuracy: 0.334531 val accuracy: 0.335000\n",
      "lr 1.000000e-07 reg 1.400000e+04 train accuracy: 0.338204 val accuracy: 0.355000\n",
      "lr 1.000000e-07 reg 1.700000e+04 train accuracy: 0.336980 val accuracy: 0.350000\n",
      "lr 1.000000e-07 reg 2.000000e+04 train accuracy: 0.339388 val accuracy: 0.345000\n",
      "lr 1.000000e-07 reg 2.300000e+04 train accuracy: 0.331061 val accuracy: 0.343000\n",
      "lr 1.000000e-07 reg 2.600000e+04 train accuracy: 0.325449 val accuracy: 0.339000\n",
      "lr 1.000000e-07 reg 2.900000e+04 train accuracy: 0.318857 val accuracy: 0.331000\n",
      "lr 1.000000e-07 reg 3.200000e+04 train accuracy: 0.320306 val accuracy: 0.332000\n",
      "lr 1.000000e-07 reg 3.500000e+04 train accuracy: 0.313776 val accuracy: 0.332000\n",
      "lr 1.000000e-07 reg 3.800000e+04 train accuracy: 0.322224 val accuracy: 0.342000\n",
      "lr 1.000000e-07 reg 4.100000e+04 train accuracy: 0.317612 val accuracy: 0.334000\n",
      "lr 1.000000e-07 reg 4.400000e+04 train accuracy: 0.312224 val accuracy: 0.321000\n",
      "lr 1.000000e-07 reg 4.700000e+04 train accuracy: 0.309204 val accuracy: 0.328000\n",
      "lr 1.000000e-07 reg 5.000000e+04 train accuracy: 0.307143 val accuracy: 0.326000\n",
      "lr 2.000000e-07 reg 5.000000e+03 train accuracy: 0.357122 val accuracy: 0.371000\n",
      "lr 2.000000e-07 reg 8.000000e+03 train accuracy: 0.360857 val accuracy: 0.375000\n",
      "lr 2.000000e-07 reg 1.100000e+04 train accuracy: 0.348796 val accuracy: 0.367000\n",
      "lr 2.000000e-07 reg 1.400000e+04 train accuracy: 0.344857 val accuracy: 0.365000\n",
      "lr 2.000000e-07 reg 1.700000e+04 train accuracy: 0.334959 val accuracy: 0.349000\n",
      "lr 2.000000e-07 reg 2.000000e+04 train accuracy: 0.336469 val accuracy: 0.353000\n",
      "lr 2.000000e-07 reg 2.300000e+04 train accuracy: 0.330551 val accuracy: 0.343000\n",
      "lr 2.000000e-07 reg 2.600000e+04 train accuracy: 0.327449 val accuracy: 0.338000\n",
      "lr 2.000000e-07 reg 2.900000e+04 train accuracy: 0.322408 val accuracy: 0.347000\n",
      "lr 2.000000e-07 reg 3.200000e+04 train accuracy: 0.326327 val accuracy: 0.335000\n",
      "lr 2.000000e-07 reg 3.500000e+04 train accuracy: 0.314306 val accuracy: 0.330000\n",
      "lr 2.000000e-07 reg 3.800000e+04 train accuracy: 0.312918 val accuracy: 0.328000\n",
      "lr 2.000000e-07 reg 4.100000e+04 train accuracy: 0.316429 val accuracy: 0.333000\n",
      "lr 2.000000e-07 reg 4.400000e+04 train accuracy: 0.304245 val accuracy: 0.325000\n",
      "lr 2.000000e-07 reg 4.700000e+04 train accuracy: 0.296327 val accuracy: 0.311000\n",
      "lr 2.000000e-07 reg 5.000000e+04 train accuracy: 0.310571 val accuracy: 0.322000\n",
      "lr 5.000000e-07 reg 5.000000e+03 train accuracy: 0.374122 val accuracy: 0.381000\n",
      "lr 5.000000e-07 reg 8.000000e+03 train accuracy: 0.360653 val accuracy: 0.377000\n",
      "lr 5.000000e-07 reg 1.100000e+04 train accuracy: 0.352449 val accuracy: 0.362000\n",
      "lr 5.000000e-07 reg 1.400000e+04 train accuracy: 0.340102 val accuracy: 0.351000\n",
      "lr 5.000000e-07 reg 1.700000e+04 train accuracy: 0.335918 val accuracy: 0.350000\n",
      "lr 5.000000e-07 reg 2.000000e+04 train accuracy: 0.329694 val accuracy: 0.346000\n",
      "lr 5.000000e-07 reg 2.300000e+04 train accuracy: 0.335878 val accuracy: 0.337000\n",
      "lr 5.000000e-07 reg 2.600000e+04 train accuracy: 0.332327 val accuracy: 0.354000\n",
      "lr 5.000000e-07 reg 2.900000e+04 train accuracy: 0.315265 val accuracy: 0.333000\n",
      "lr 5.000000e-07 reg 3.200000e+04 train accuracy: 0.314286 val accuracy: 0.332000\n",
      "lr 5.000000e-07 reg 3.500000e+04 train accuracy: 0.307000 val accuracy: 0.327000\n",
      "lr 5.000000e-07 reg 3.800000e+04 train accuracy: 0.316510 val accuracy: 0.333000\n",
      "lr 5.000000e-07 reg 4.100000e+04 train accuracy: 0.306612 val accuracy: 0.320000\n",
      "lr 5.000000e-07 reg 4.400000e+04 train accuracy: 0.309020 val accuracy: 0.331000\n",
      "lr 5.000000e-07 reg 4.700000e+04 train accuracy: 0.316694 val accuracy: 0.332000\n",
      "lr 5.000000e-07 reg 5.000000e+04 train accuracy: 0.305000 val accuracy: 0.316000\n",
      "lr 1.000000e-06 reg 5.000000e+03 train accuracy: 0.368673 val accuracy: 0.389000\n",
      "lr 1.000000e-06 reg 8.000000e+03 train accuracy: 0.354224 val accuracy: 0.363000\n",
      "lr 1.000000e-06 reg 1.100000e+04 train accuracy: 0.344816 val accuracy: 0.350000\n",
      "lr 1.000000e-06 reg 1.400000e+04 train accuracy: 0.340286 val accuracy: 0.349000\n",
      "lr 1.000000e-06 reg 1.700000e+04 train accuracy: 0.338490 val accuracy: 0.367000\n",
      "lr 1.000000e-06 reg 2.000000e+04 train accuracy: 0.321143 val accuracy: 0.327000\n",
      "lr 1.000000e-06 reg 2.300000e+04 train accuracy: 0.317837 val accuracy: 0.328000\n",
      "lr 1.000000e-06 reg 2.600000e+04 train accuracy: 0.309490 val accuracy: 0.313000\n",
      "lr 1.000000e-06 reg 2.900000e+04 train accuracy: 0.299857 val accuracy: 0.329000\n",
      "lr 1.000000e-06 reg 3.200000e+04 train accuracy: 0.306000 val accuracy: 0.320000\n",
      "lr 1.000000e-06 reg 3.500000e+04 train accuracy: 0.305857 val accuracy: 0.307000\n",
      "lr 1.000000e-06 reg 3.800000e+04 train accuracy: 0.295918 val accuracy: 0.306000\n",
      "lr 1.000000e-06 reg 4.100000e+04 train accuracy: 0.299327 val accuracy: 0.315000\n",
      "lr 1.000000e-06 reg 4.400000e+04 train accuracy: 0.306000 val accuracy: 0.315000\n",
      "lr 1.000000e-06 reg 4.700000e+04 train accuracy: 0.312245 val accuracy: 0.330000\n",
      "lr 1.000000e-06 reg 5.000000e+04 train accuracy: 0.289224 val accuracy: 0.292000\n",
      "best validation accuracy achieved during cross-validation: 0.389000\n"
     ]
    }
   ],
   "source": [
    "# Use the validation set to tune hyperparameters (regularization strength and\n",
    "# learning rate). You should experiment with different ranges for the learning\n",
    "# rates and regularization strengths; if you are careful you should be able to\n",
    "# get a classification accuracy of over 0.35 on the validation set.\n",
    "from cs231n.classifiers import Softmax\n",
    "results = {}\n",
    "best_val = -1\n",
    "best_softmax = None\n",
    "# learning_rates = [1e-7, 5e-7]\n",
    "learning_rates = [5e-8, 1e-7, 2e-7, 5e-7, 1e-6]\n",
    "# regularization_strengths = [2.5e4, 5e4]\n",
    "regularization_strengths = [i*1e3 for i in range(5, 51, 3)]\n",
    "\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Use the validation set to set the learning rate and regularization strength. #\n",
    "# This should be identical to the validation that you did for the SVM; save    #\n",
    "# the best trained softmax classifer in best_softmax.                          #\n",
    "################################################################################\n",
    "for learning_rate in learning_rates:\n",
    "    for reg in regularization_strengths:\n",
    "        sm_clf = Softmax()\n",
    "        sm_clf.train(X_train, y_train, learning_rate=learning_rate, reg=reg,\n",
    "                      num_iters=1000, verbose=True)\n",
    "        \n",
    "        y_train_pred = sm_clf.predict(X_train)\n",
    "        training_accuracy = np.mean(y_train == y_train_pred)\n",
    "\n",
    "        y_val_pred = sm_clf.predict(X_val)\n",
    "        validation_accuracy = np.mean(y_val == y_val_pred)\n",
    "\n",
    "        results[(learning_rate, reg)] = (training_accuracy, validation_accuracy)\n",
    "        \n",
    "        if validation_accuracy > best_val:\n",
    "            best_val = validation_accuracy\n",
    "            best_softmax = sm_clf\n",
    "\n",
    "################################################################################\n",
    "#                              END OF YOUR CODE                                #\n",
    "################################################################################\n",
    "    \n",
    "# Print out results.\n",
    "for lr, reg in sorted(results):\n",
    "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
    "    print('lr %e reg %e train accuracy: %f val accuracy: %f' % (\n",
    "                lr, reg, train_accuracy, val_accuracy))\n",
    "    \n",
    "print('best validation accuracy achieved during cross-validation: %f' % best_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax on raw pixels final test set accuracy: 0.385000\n"
     ]
    }
   ],
   "source": [
    "# evaluate on test set\n",
    "# Evaluate the best softmax on test set\n",
    "y_test_pred = best_softmax.predict(X_test)\n",
    "test_accuracy = np.mean(y_test == y_test_pred)\n",
    "print('softmax on raw pixels final test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inline Question** - *True or False*\n",
    "\n",
    "It's possible to add a new datapoint to a training set that would leave the SVM loss unchanged, but this is not the case with the Softmax classifier loss.\n",
    "\n",
    "*Your answer*:\n",
    "\n",
    "*Your explanation*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10dbf3da0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the learned weights for each class\n",
    "w = best_softmax.W[:-1,:] # strip out the bias\n",
    "w = w.reshape(32, 32, 3, 10)\n",
    "\n",
    "w_min, w_max = np.min(w), np.max(w)\n",
    "\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    \n",
    "    # Rescale the weights to be between 0 and 255\n",
    "    wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n",
    "    plt.imshow(wimg.astype('uint8'))\n",
    "    plt.axis('off')\n",
    "    plt.title(classes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
